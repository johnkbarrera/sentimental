{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idUsuario</th>\n",
       "      <th>fechaCreacion</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1062491062751686656</td>\n",
       "      <td>226802593</td>\n",
       "      <td>2018-11-13 23:43:04</td>\n",
       "      <td>Como cuando ahora también tiene que prepararte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1062491070980935682</td>\n",
       "      <td>1020688128250646528</td>\n",
       "      <td>2018-11-13 23:43:06</td>\n",
       "      <td>Sres. @AsambleaEcuador no arriesguen más la sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1062491474594476039</td>\n",
       "      <td>353815647</td>\n",
       "      <td>2018-11-13 23:44:42</td>\n",
       "      <td>Comencemos esta experiencia!! #psicologosenelm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1062491477002084352</td>\n",
       "      <td>561395391</td>\n",
       "      <td>2018-11-13 23:44:42</td>\n",
       "      <td>@galarzaramiro1 @AdrianPalacioJ @VillaFernando...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id            idUsuario       fechaCreacion  \\\n",
       "0  1062491062751686656            226802593 2018-11-13 23:43:04   \n",
       "1  1062491070980935682  1020688128250646528 2018-11-13 23:43:06   \n",
       "2  1062491474594476039            353815647 2018-11-13 23:44:42   \n",
       "3  1062491477002084352            561395391 2018-11-13 23:44:42   \n",
       "\n",
       "                                               texto  \n",
       "0  Como cuando ahora también tiene que prepararte...  \n",
       "1  Sres. @AsambleaEcuador no arriesguen más la sa...  \n",
       "2  Comencemos esta experiencia!! #psicologosenelm...  \n",
       "3  @galarzaramiro1 @AdrianPalacioJ @VillaFernando...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "tweetSchema= {'fechaCreacion': {'format': '%Y-%m-%d %H:%M:%S'}}\n",
    "\n",
    "myconnection = pymysql.connect(host = 'localhost', user = 'root',password = 'root',db = 'dbtwitter', charset='utf8')\n",
    "\n",
    "data_db = None\n",
    "try:\n",
    "    query = \"\"\"SELECT * FROM tweet\"\"\"\n",
    "    data_db = pd.read_sql(query, con = myconnection, parse_dates = tweetSchema)\n",
    "finally:\n",
    "    myconnection.close()\n",
    "\n",
    "data_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_texto = data_db[\"texto\"]\n",
    "# df_tweet_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros', 'vosostras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened', 'rt', 'aun', 'oe']\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~¿\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('spanish')\n",
    "stop_words.extend([\"rt\", \"aun\", \"oe\"])\n",
    "exclude = string.punctuation\n",
    "exclude = exclude + \"¿\"\n",
    "\n",
    "lemma = SnowballStemmer('spanish')\n",
    "\n",
    "print(stop_words)\n",
    "print(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "##def remove_number(list_text):\n",
    "##    return ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "def remove_three_dots(list_text):\n",
    "    return [re.sub(r\"[a-zA-Z]+(\\……|\\…)$\", \" \", texto) for texto in list_text]\n",
    "\n",
    "def remove_url(list_text):\n",
    "    return [re.sub(r\"http\\S+\", \"\", texto).strip() for texto in list_text]\n",
    "\n",
    "def remove_breakline(list_text):\n",
    "    return [re.sub('\\s+', ' ', texto) for texto in list_text]\n",
    "\n",
    "def remove_single_quotes(list_text):\n",
    "    return [re.sub(\"\\'\", \"\", texto) for texto in list_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Como cuando ahora también tiene que prepararte psicológicamente para el torneo #marcianogaming #PUBGMOBILE… https://t.co/buM1Bn6dqE', 'Sres. @AsambleaEcuador no arriesguen más la salud de los jóvenes mediante el uso “medicinal” de marihuana. Sin estu… https://t.co/u3DKINYIcf', 'Comencemos esta experiencia!! #psicologosenelmundo #psicologovenezolano #reclutamientoyseleccion… https://t.co/EGMyHDcGij', '@galarzaramiro1 @AdrianPalacioJ @VillaFernando_ Debería pedirle el  crédito a AP; tienen para darnos fácil unos 20B… https://t.co/trwLssbhfC']\n"
     ]
    }
   ],
   "source": [
    "data = data_db.texto.values.tolist()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['como', 'cuando', 'ahora', 'tambien', 'tiene', 'que', 'prepararte', 'para', 'el', 'torneo', 'marcianogaming'], ['sres', 'asambleaecuador', 'no', 'arriesguen', 'mas', 'la', 'salud', 'de', 'los', 'jovenes', 'mediante', 'el', 'uso', 'medicinal', 'de', 'marihuana', 'sin'], ['comencemos', 'esta', 'experiencia'], ['galarzaramiro', 'adrianpalacioj', 'villafernando_', 'deberia', 'pedirle', 'el', 'credito', 'ap', 'tienen', 'para', 'darnos', 'facil', 'unos']]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "txt_free_url = remove_url(data)\n",
    "txt_free_threedot = remove_three_dots(txt_free_url)\n",
    "txt_free_breakline = remove_breakline(txt_free_threedot)\n",
    "txt_free_singlequotes = remove_single_quotes(txt_free_breakline)\n",
    "\n",
    "def texto_a_palabras(texto:str):\n",
    "    for sentencia in texto:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentencia), deacc=True))\n",
    "        \n",
    "data_palabras = list(texto_a_palabras(txt_free_singlequotes))\n",
    "\n",
    "print(data_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkn/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(data_palabras, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_palabras], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comencemos', 'esta', 'experiencia']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_mod[data_palabras[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ahora', 'tambien', 'prepararte', 'tornear', 'marcianogaming'], ['sres', 'asambleaecuador', 'arriesgar', 'mas', 'salud', 'jovenes', 'usar', 'medicinal', 'marihuana'], ['comenzar', 'experiencia'], ['galarzaramiro', 'adrianpalacioj', 'villafernando', '_', 'deberia', 'pedirle', 'credito', 'ap', 'darnos', 'facil']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# python -m spacy download es\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_palabras)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('es', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    " \n",
    "diccionario = corpora.Dictionary(data_lemmatized)\n",
    "#Corpus\n",
    "texto = data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comencemos', 'experiencia']\n",
      "[(14, 1), (15, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [diccionario.doc2bow(doc) for doc in texto]\n",
    "\n",
    "print(data_words_bigrams[2])\n",
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sres', 'asambleaecuador', 'arriesguen', 'mas', 'salud', 'jovenes', 'mediante', 'uso', 'medicinal', 'marihuana']\n",
      "[(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n"
     ]
    }
   ],
   "source": [
    "print (data_words_bigrams[1])\n",
    "print (corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ahora', 1),\n",
       "  ('marcianogaming', 1),\n",
       "  ('prepararte', 1),\n",
       "  ('tambien', 1),\n",
       "  ('tornear', 1)],\n",
       " [('arriesgar', 1),\n",
       "  ('asambleaecuador', 1),\n",
       "  ('jovenes', 1),\n",
       "  ('marihuana', 1),\n",
       "  ('mas', 1),\n",
       "  ('medicinal', 1),\n",
       "  ('salud', 1),\n",
       "  ('sres', 1),\n",
       "  ('usar', 1)],\n",
       " [('comenzar', 1), ('experiencia', 1)]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(diccionario[id], freq) for id, freq in cp] for cp in corpus[0:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(list(diccionario.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correr y entrenar el modelo LDA sobre la matriz de términos.\n",
    "lda_model = gensim.models.LdaModel(corpus,\n",
    "                                   num_topics=20,\n",
    "                                   id2word = diccionario,\n",
    "                                   random_state=100,\n",
    "                                   passes=10,\n",
    "                                   update_every=1,\n",
    "                                   chunksize=100,\n",
    "                                   alpha='auto',\n",
    "                                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 1 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 2 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 3 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 4 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 5 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 6 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 7 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 8 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 9 Word: 0.318*\"experiencia\" + 0.318*\"comenzar\" + 0.015*\"ap\" + 0.015*\"darnos\" + 0.015*\"_\" + 0.015*\"adrianpalacioj\" + 0.015*\"pedirle\" + 0.015*\"credito\" + 0.015*\"sres\" + 0.015*\"galarzaramiro\"\n",
      "\n",
      "Topic: 10 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 11 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 12 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 13 Word: 0.102*\"asambleaecuador\" + 0.102*\"arriesgar\" + 0.102*\"mas\" + 0.102*\"marihuana\" + 0.102*\"sres\" + 0.102*\"salud\" + 0.102*\"usar\" + 0.102*\"jovenes\" + 0.102*\"medicinal\" + 0.005*\"deberia\"\n",
      "\n",
      "Topic: 14 Word: 0.093*\"_\" + 0.093*\"deberia\" + 0.093*\"pedirle\" + 0.093*\"galarzaramiro\" + 0.093*\"facil\" + 0.093*\"villafernando\" + 0.093*\"darnos\" + 0.093*\"credito\" + 0.093*\"ap\" + 0.093*\"adrianpalacioj\"\n",
      "\n",
      "Topic: 15 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 16 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 17 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 18 Word: 0.038*\"credito\" + 0.038*\"deberia\" + 0.038*\"_\" + 0.038*\"adrianpalacioj\" + 0.038*\"ap\" + 0.038*\"comenzar\" + 0.038*\"darnos\" + 0.038*\"pedirle\" + 0.038*\"galarzaramiro\" + 0.038*\"sres\"\n",
      "\n",
      "Topic: 19 Word: 0.167*\"prepararte\" + 0.167*\"tambien\" + 0.167*\"tornear\" + 0.167*\"marcianogaming\" + 0.167*\"ahora\" + 0.008*\"credito\" + 0.008*\"galarzaramiro\" + 0.008*\"ap\" + 0.008*\"_\" + 0.008*\"darnos\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics():\n",
    "    print('Topic: {} Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(26 unique tokens: ['ahora', 'marcianogaming', 'prepararte', 'tambien', 'tornear']...)\n"
     ]
    }
   ],
   "source": [
    "# pip install pyLDAvis\n",
    "\n",
    "import warnings\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(lda_model,corpus,diccionario)\n",
    "vis\n",
    "\n",
    "print(diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
